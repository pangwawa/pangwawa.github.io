---
title: "Clickhouse基础概念与架构原理"
tags: []
author: "Jack Wu"
date: 2020-11-02T11:22:51+08:00
draft: false
---

### 什么是ClickHouse
ClickHouse是面向联机分析处理的列式数据库，支持SQL查询，且查询性能好，特别是基于大宽表的聚合分析查询性能非常优异，比其他分析型数据库速度快一个数量级。
ClickHouse不单单是一个数据库， 它是一个数据库管理系统。因为它允许在运行时创建表和数据库、加载数据和运行查询，而无需重新配置或重启服务。
主要特性包括：
    数据压缩比高。
    多核并行计算。
    向量化计算引擎。
    支持嵌套数据结构。
    支持稀疏索引。
    
支持数据Insert和Update。


 #### clickhouse为什么如此快
      
  1）优秀的代码，对性能的极致追求
  
  clickhouse是CPP编写的，代码中大量使用了CPP最新的特性来对查询进行加速。
  
  2）优秀的执行引擎以及存储引擎
  
  clickhouse是基于列式存储的，使用了向量化的执行引擎，利用SIMD指令进行处理加速，同时使用LLVM加快函数编译执行，当然了Presto也大量的使用了这样的特性。
  
  3）稀疏索引
  
  相比于传统基于HDFS的OLAP引擎，clickhouse不仅有基于分区的过滤，还有基于列级别的稀疏索引，这样在进行条件查询的时候可以过滤到很多不需要扫描的块，这样对提升查询速度是很有帮助的。
  
  4）存储执行耦合
  
  存储和执行分离是一个趋势，但是存储和执行耦合也是有优势的，避免了网络的开销，CPU的极致压榨加上SSD的加持，每秒的数据传输对于网络带宽的压力是非常大的，耦合部署可以避免该问题。
  
  5）数据存储在SSD，极高的iops。
 
 #### Clickhouse高性能存储和查询的实现原理
 
 1、多核CPU并行计算，看到刚才的文件存储方式，文件是按照压缩块的方式存在.bin文件中。ClickHouse可以通过大量CPU，并行读取不同的压缩块并行解压计算。
 
 2、SIMD指令集加速，充分利用了CPU寄存器的并行计算能力。如果是传统的CPU指令集，寄存器就算有128位，如果要进行N次8bit的计算，每次都只能利用寄存器的低8位重复计算N次；而SIMD的话，可以将16个8bit的运算并行放到128位的寄存器中，仅通过1次计算就可以并行计算16个8bit的运算。ClickHouse是通过SSE2指令集实现的，所以最好选用Intel的机器。
 
 3、之前介绍的，ClickHouse分布式水平扩展集群的能力，也可以很好的提升性能。
 
 4、还有稀疏索引，列式存储和极致的数据压缩，也都是大数据场景下高性能查询的关键点。
 
 5、实际上，ClickHouse还有很多优化性能的细节。比如聚合分析，通常的实现是通过HashMap实现，HashMap的key是group by的key，HashMap的value是聚合的值。ClickHouse通过对长字符串的Key，进行Hash实现的HashMapWithSavedHash，以及对Uint8这类范围小的数据字段通过数组实现的FixedHashMap，以及大量新增类别字段和内存限制的场景，也有TwoLevelHashMap和Split to disk的实现方案。
 
 实际上，ClickHouse的内核实现中，并没有使用什么神秘的算法，但是正是这所有的优化组合在一起，才有了ClickHouse彪悍的查询性能。
 
 #### 海量数据直接写入ClickHouse会失败
 
 ClickHouse的MergeTree表引擎，底层原理是类似于LSM-tree。数据通过Append的方式写入，后续再启动merge线程将小的数据文件进行合并。
 
 一次数据写入，会生成一个文件目录。目录结构可以看到，分为四个部分：
 
 第一部分，是分区ID的信息；
 
 第二部分，是这个目录中包含数据的最小BlockNum；
 
 第三部分，是这个目录中包含数据的最大BlockNum；
 
 第四部分，是这个目录进行合并的等级。
 
 举个例子，图中两个黄色的数据目录合并成蓝色的数据目录，数据BlockNum从1_1和2_2合并成了1_2，数据合并level也从0变成了1。
 
 了解了ClickHouse MergeTree家族表的写入过程，这里我们就会发现两个问题。
 
 第一：如果一次写入的数据量太小，比如一条写一次，那么会产生大量的文件目录。当后台合并线程来不及合并的时候，文件目录数量会越来越多，这会导致ClickHouse抛出Too many parts的异常，写入失败。
 第二：根据之前的介绍，每一次写入除了写入数据本身，ClickHouse还需要跟Zookeeper进行十来次的数据交互，而我们知道Zookeeper是不能承受高并发的访问。可以看到，写入QPS过高导致进一步Zookeeper的QPS过高，从而导致Zookeeper崩溃。
 我们采用的解决方案是，改用Batch的方式写入。即一个Batch的数据，产生一个数据目录，与Zookeeper进行一系列交互。那Batch设置多大呢，Batch太小的话缓解不了ZK的压力，Batch也不能太大，不然上游内存的压力和数据延迟会太大。通过实验，最终我们选用了大小几十万的Batch。
 
 这样避免了QPS太高带来的问题。当前方案其实还有优化空间的，比如Zookeeper是无法线性扩展的。我了解到，业内有些团队就把log和data part相关信息不写入Zookeeper，这样减少了Zookeeper的压力。不过这样涉及到了对源代码的修改，对于一般的业务团队实现的成本太高了。
 
 数据写入，遇到的第二个问题是，如果数据写入通过分布式表写入会遇到单点问题。
 
 先介绍一下分布式表。分布式表实际上是一张逻辑表并不存储真实数据，可以理解为一张代理表。
 
 比如用户查询分布式表，分布式表会将查询请求下发到每一个分片的本地表上进行查询，然后收集每个分片本地表的结果，汇总之后再返回给用户。
 
 用户写入分布式表的场景，是用户将一个Batch的数据写入分布式表，分布式表根据一定的规则，将这个Batch的数据分为若干个Mini Batch的数据，存储到不同的分片上。
 
 这里有一个很容易误解的地方，我们开始以为，分布式表是按照一定规则做一个网络转发，那么我们当时想只要万兆网卡的带宽足够，就不会出现单点的性能瓶颈。
 
 尤其是Clickhouse底层运用的是Mergetree，在合并的过程中，会存在写放大的问题加重磁盘的压力。峰值每分钟几千万条数据写完耗时几十秒，如果正在做Merge就会阻塞写入请求，查询也会非常慢。
 
 我们做的两个优化方案：
 
 第一，对磁盘做Raid提升磁盘的IO。
 
 第二，在写入之前，上游进行数据划分分表操作，直接分开写入到不同的分片上，磁盘压力直接变为了原来的1/N。
 
 这样很好的避免了磁盘的单点瓶颈。