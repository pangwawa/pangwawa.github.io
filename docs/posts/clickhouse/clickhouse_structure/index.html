<!doctype html>
<html lang="zh-Hans">
  <head>
    <title>Clickhouse基础概念与架构原理 // 小吴的工作手记</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.76.5" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Jack Wu" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://pangwawa.github.io/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css" />
    <link rel="Shortcut Icon" href="favicon.ico" type="image/x-icon" />
    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Clickhouse基础概念与架构原理"/>
<meta name="twitter:description" content="什么是ClickHouse ClickHouse是面向联机分析处理的列式数据库，支持SQL查询，且查询性能好，特别是基于大宽表的聚合分析查询性能非常优异，比其他分析型数据库速度快一个数量级。 ClickHouse不单单是一个数据库， 它是一个数据库管理系统。因为它允许在运行时创建表和数据库、加载数据和运行查询，而无需重新配置或重启服务。 主要特性包括： 数据压缩比高。 多核并行计算。 向量化计算引擎。 支持嵌套数据结构。 支持稀疏索引。
支持数据Insert和Update。
clickhouse为什么如此快 1）优秀的代码，对性能的极致追求
clickhouse是CPP编写的，代码中大量使用了CPP最新的特性来对查询进行加速。
2）优秀的执行引擎以及存储引擎
clickhouse是基于列式存储的，使用了向量化的执行引擎，利用SIMD指令进行处理加速，同时使用LLVM加快函数编译执行，当然了Presto也大量的使用了这样的特性。
3）稀疏索引
相比于传统基于HDFS的OLAP引擎，clickhouse不仅有基于分区的过滤，还有基于列级别的稀疏索引，这样在进行条件查询的时候可以过滤到很多不需要扫描的块，这样对提升查询速度是很有帮助的。
4）存储执行耦合
存储和执行分离是一个趋势，但是存储和执行耦合也是有优势的，避免了网络的开销，CPU的极致压榨加上SSD的加持，每秒的数据传输对于网络带宽的压力是非常大的，耦合部署可以避免该问题。
5）数据存储在SSD，极高的iops。
Clickhouse高性能存储和查询的实现原理 1、多核CPU并行计算，看到刚才的文件存储方式，文件是按照压缩块的方式存在.bin文件中。ClickHouse可以通过大量CPU，并行读取不同的压缩块并行解压计算。
2、SIMD指令集加速，充分利用了CPU寄存器的并行计算能力。如果是传统的CPU指令集，寄存器就算有128位，如果要进行N次8bit的计算，每次都只能利用寄存器的低8位重复计算N次；而SIMD的话，可以将16个8bit的运算并行放到128位的寄存器中，仅通过1次计算就可以并行计算16个8bit的运算。ClickHouse是通过SSE2指令集实现的，所以最好选用Intel的机器。
3、之前介绍的，ClickHouse分布式水平扩展集群的能力，也可以很好的提升性能。
4、还有稀疏索引，列式存储和极致的数据压缩，也都是大数据场景下高性能查询的关键点。
5、实际上，ClickHouse还有很多优化性能的细节。比如聚合分析，通常的实现是通过HashMap实现，HashMap的key是group by的key，HashMap的value是聚合的值。ClickHouse通过对长字符串的Key，进行Hash实现的HashMapWithSavedHash，以及对Uint8这类范围小的数据字段通过数组实现的FixedHashMap，以及大量新增类别字段和内存限制的场景，也有TwoLevelHashMap和Split to disk的实现方案。
实际上，ClickHouse的内核实现中，并没有使用什么神秘的算法，但是正是这所有的优化组合在一起，才有了ClickHouse彪悍的查询性能。
海量数据直接写入ClickHouse会失败 ClickHouse的MergeTree表引擎，底层原理是类似于LSM-tree。数据通过Append的方式写入，后续再启动merge线程将小的数据文件进行合并。
一次数据写入，会生成一个文件目录。目录结构可以看到，分为四个部分：
第一部分，是分区ID的信息；
第二部分，是这个目录中包含数据的最小BlockNum；
第三部分，是这个目录中包含数据的最大BlockNum；
第四部分，是这个目录进行合并的等级。
举个例子，图中两个黄色的数据目录合并成蓝色的数据目录，数据BlockNum从1_1和2_2合并成了1_2，数据合并level也从0变成了1。
了解了ClickHouse MergeTree家族表的写入过程，这里我们就会发现两个问题。
第一：如果一次写入的数据量太小，比如一条写一次，那么会产生大量的文件目录。当后台合并线程来不及合并的时候，文件目录数量会越来越多，这会导致ClickHouse抛出Too many parts的异常，写入失败。 第二：根据之前的介绍，每一次写入除了写入数据本身，ClickHouse还需要跟Zookeeper进行十来次的数据交互，而我们知道Zookeeper是不能承受高并发的访问。可以看到，写入QPS过高导致进一步Zookeeper的QPS过高，从而导致Zookeeper崩溃。 我们采用的解决方案是，改用Batch的方式写入。即一个Batch的数据，产生一个数据目录，与Zookeeper进行一系列交互。那Batch设置多大呢，Batch太小的话缓解不了ZK的压力，Batch也不能太大，不然上游内存的压力和数据延迟会太大。通过实验，最终我们选用了大小几十万的Batch。
这样避免了QPS太高带来的问题。当前方案其实还有优化空间的，比如Zookeeper是无法线性扩展的。我了解到，业内有些团队就把log和data part相关信息不写入Zookeeper，这样减少了Zookeeper的压力。不过这样涉及到了对源代码的修改，对于一般的业务团队实现的成本太高了。
数据写入，遇到的第二个问题是，如果数据写入通过分布式表写入会遇到单点问题。
先介绍一下分布式表。分布式表实际上是一张逻辑表并不存储真实数据，可以理解为一张代理表。
比如用户查询分布式表，分布式表会将查询请求下发到每一个分片的本地表上进行查询，然后收集每个分片本地表的结果，汇总之后再返回给用户。
用户写入分布式表的场景，是用户将一个Batch的数据写入分布式表，分布式表根据一定的规则，将这个Batch的数据分为若干个Mini Batch的数据，存储到不同的分片上。
这里有一个很容易误解的地方，我们开始以为，分布式表是按照一定规则做一个网络转发，那么我们当时想只要万兆网卡的带宽足够，就不会出现单点的性能瓶颈。
尤其是Clickhouse底层运用的是Mergetree，在合并的过程中，会存在写放大的问题加重磁盘的压力。峰值每分钟几千万条数据写完耗时几十秒，如果正在做Merge就会阻塞写入请求，查询也会非常慢。
我们做的两个优化方案：
第一，对磁盘做Raid提升磁盘的IO。
第二，在写入之前，上游进行数据划分分表操作，直接分开写入到不同的分片上，磁盘压力直接变为了原来的1/N。
这样很好的避免了磁盘的单点瓶颈。"/>

    <meta property="og:title" content="Clickhouse基础概念与架构原理" />
<meta property="og:description" content="什么是ClickHouse ClickHouse是面向联机分析处理的列式数据库，支持SQL查询，且查询性能好，特别是基于大宽表的聚合分析查询性能非常优异，比其他分析型数据库速度快一个数量级。 ClickHouse不单单是一个数据库， 它是一个数据库管理系统。因为它允许在运行时创建表和数据库、加载数据和运行查询，而无需重新配置或重启服务。 主要特性包括： 数据压缩比高。 多核并行计算。 向量化计算引擎。 支持嵌套数据结构。 支持稀疏索引。
支持数据Insert和Update。
clickhouse为什么如此快 1）优秀的代码，对性能的极致追求
clickhouse是CPP编写的，代码中大量使用了CPP最新的特性来对查询进行加速。
2）优秀的执行引擎以及存储引擎
clickhouse是基于列式存储的，使用了向量化的执行引擎，利用SIMD指令进行处理加速，同时使用LLVM加快函数编译执行，当然了Presto也大量的使用了这样的特性。
3）稀疏索引
相比于传统基于HDFS的OLAP引擎，clickhouse不仅有基于分区的过滤，还有基于列级别的稀疏索引，这样在进行条件查询的时候可以过滤到很多不需要扫描的块，这样对提升查询速度是很有帮助的。
4）存储执行耦合
存储和执行分离是一个趋势，但是存储和执行耦合也是有优势的，避免了网络的开销，CPU的极致压榨加上SSD的加持，每秒的数据传输对于网络带宽的压力是非常大的，耦合部署可以避免该问题。
5）数据存储在SSD，极高的iops。
Clickhouse高性能存储和查询的实现原理 1、多核CPU并行计算，看到刚才的文件存储方式，文件是按照压缩块的方式存在.bin文件中。ClickHouse可以通过大量CPU，并行读取不同的压缩块并行解压计算。
2、SIMD指令集加速，充分利用了CPU寄存器的并行计算能力。如果是传统的CPU指令集，寄存器就算有128位，如果要进行N次8bit的计算，每次都只能利用寄存器的低8位重复计算N次；而SIMD的话，可以将16个8bit的运算并行放到128位的寄存器中，仅通过1次计算就可以并行计算16个8bit的运算。ClickHouse是通过SSE2指令集实现的，所以最好选用Intel的机器。
3、之前介绍的，ClickHouse分布式水平扩展集群的能力，也可以很好的提升性能。
4、还有稀疏索引，列式存储和极致的数据压缩，也都是大数据场景下高性能查询的关键点。
5、实际上，ClickHouse还有很多优化性能的细节。比如聚合分析，通常的实现是通过HashMap实现，HashMap的key是group by的key，HashMap的value是聚合的值。ClickHouse通过对长字符串的Key，进行Hash实现的HashMapWithSavedHash，以及对Uint8这类范围小的数据字段通过数组实现的FixedHashMap，以及大量新增类别字段和内存限制的场景，也有TwoLevelHashMap和Split to disk的实现方案。
实际上，ClickHouse的内核实现中，并没有使用什么神秘的算法，但是正是这所有的优化组合在一起，才有了ClickHouse彪悍的查询性能。
海量数据直接写入ClickHouse会失败 ClickHouse的MergeTree表引擎，底层原理是类似于LSM-tree。数据通过Append的方式写入，后续再启动merge线程将小的数据文件进行合并。
一次数据写入，会生成一个文件目录。目录结构可以看到，分为四个部分：
第一部分，是分区ID的信息；
第二部分，是这个目录中包含数据的最小BlockNum；
第三部分，是这个目录中包含数据的最大BlockNum；
第四部分，是这个目录进行合并的等级。
举个例子，图中两个黄色的数据目录合并成蓝色的数据目录，数据BlockNum从1_1和2_2合并成了1_2，数据合并level也从0变成了1。
了解了ClickHouse MergeTree家族表的写入过程，这里我们就会发现两个问题。
第一：如果一次写入的数据量太小，比如一条写一次，那么会产生大量的文件目录。当后台合并线程来不及合并的时候，文件目录数量会越来越多，这会导致ClickHouse抛出Too many parts的异常，写入失败。 第二：根据之前的介绍，每一次写入除了写入数据本身，ClickHouse还需要跟Zookeeper进行十来次的数据交互，而我们知道Zookeeper是不能承受高并发的访问。可以看到，写入QPS过高导致进一步Zookeeper的QPS过高，从而导致Zookeeper崩溃。 我们采用的解决方案是，改用Batch的方式写入。即一个Batch的数据，产生一个数据目录，与Zookeeper进行一系列交互。那Batch设置多大呢，Batch太小的话缓解不了ZK的压力，Batch也不能太大，不然上游内存的压力和数据延迟会太大。通过实验，最终我们选用了大小几十万的Batch。
这样避免了QPS太高带来的问题。当前方案其实还有优化空间的，比如Zookeeper是无法线性扩展的。我了解到，业内有些团队就把log和data part相关信息不写入Zookeeper，这样减少了Zookeeper的压力。不过这样涉及到了对源代码的修改，对于一般的业务团队实现的成本太高了。
数据写入，遇到的第二个问题是，如果数据写入通过分布式表写入会遇到单点问题。
先介绍一下分布式表。分布式表实际上是一张逻辑表并不存储真实数据，可以理解为一张代理表。
比如用户查询分布式表，分布式表会将查询请求下发到每一个分片的本地表上进行查询，然后收集每个分片本地表的结果，汇总之后再返回给用户。
用户写入分布式表的场景，是用户将一个Batch的数据写入分布式表，分布式表根据一定的规则，将这个Batch的数据分为若干个Mini Batch的数据，存储到不同的分片上。
这里有一个很容易误解的地方，我们开始以为，分布式表是按照一定规则做一个网络转发，那么我们当时想只要万兆网卡的带宽足够，就不会出现单点的性能瓶颈。
尤其是Clickhouse底层运用的是Mergetree，在合并的过程中，会存在写放大的问题加重磁盘的压力。峰值每分钟几千万条数据写完耗时几十秒，如果正在做Merge就会阻塞写入请求，查询也会非常慢。
我们做的两个优化方案：
第一，对磁盘做Raid提升磁盘的IO。
第二，在写入之前，上游进行数据划分分表操作，直接分开写入到不同的分片上，磁盘压力直接变为了原来的1/N。
这样很好的避免了磁盘的单点瓶颈。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://pangwawa.github.io/posts/clickhouse/clickhouse_structure/" />
<meta property="article:published_time" content="2020-11-02T11:22:51+08:00" />
<meta property="article:modified_time" content="2020-11-02T11:22:51+08:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://pangwawa.github.io"><img class="app-header-avatar" style="border: 0.1rem none;" src="/avatar.jpg" alt="Jack Wu" /></a>
      <h2>小吴的工作手记</h2>
      
      <nav  class="app-header-menu" style="font-size: 20px">
        <div style="margin-top: 10px">
          <a href="/">Home</a>
        </div>
        <div style="margin-top: 10px">
          <a href="/tags/">Tags</a>
        </div>
        <div style="margin-top: 10px">
          <a href="/about/about_me/">About</a>
        </div>
      </nav>
      <p>Stay hungry, stay foolish.</p>
      <p style="">Email：jackwu1024@163.com</p>
      
      <div class="app-header-social" style="margin-top: 20px">
        
        <a target="_blank" href="https://github.com/pangwawa" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
        <a target="_blank" href="https://blog.csdn.net/Jack__iT" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-twitter">
  <title>twitter</title>
  <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Clickhouse基础概念与架构原理</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Nov 2, 2020
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          1 min read
        </div>
      </div>
    </header>
    <div class="post-content">
      <h3 id="什么是clickhouse">什么是ClickHouse</h3>
<p>ClickHouse是面向联机分析处理的列式数据库，支持SQL查询，且查询性能好，特别是基于大宽表的聚合分析查询性能非常优异，比其他分析型数据库速度快一个数量级。
ClickHouse不单单是一个数据库， 它是一个数据库管理系统。因为它允许在运行时创建表和数据库、加载数据和运行查询，而无需重新配置或重启服务。
主要特性包括：
数据压缩比高。
多核并行计算。
向量化计算引擎。
支持嵌套数据结构。
支持稀疏索引。</p>
<p>支持数据Insert和Update。</p>
<h4 id="clickhouse为什么如此快">clickhouse为什么如此快</h4>
<p>1）优秀的代码，对性能的极致追求</p>
<p>clickhouse是CPP编写的，代码中大量使用了CPP最新的特性来对查询进行加速。</p>
<p>2）优秀的执行引擎以及存储引擎</p>
<p>clickhouse是基于列式存储的，使用了向量化的执行引擎，利用SIMD指令进行处理加速，同时使用LLVM加快函数编译执行，当然了Presto也大量的使用了这样的特性。</p>
<p>3）稀疏索引</p>
<p>相比于传统基于HDFS的OLAP引擎，clickhouse不仅有基于分区的过滤，还有基于列级别的稀疏索引，这样在进行条件查询的时候可以过滤到很多不需要扫描的块，这样对提升查询速度是很有帮助的。</p>
<p>4）存储执行耦合</p>
<p>存储和执行分离是一个趋势，但是存储和执行耦合也是有优势的，避免了网络的开销，CPU的极致压榨加上SSD的加持，每秒的数据传输对于网络带宽的压力是非常大的，耦合部署可以避免该问题。</p>
<p>5）数据存储在SSD，极高的iops。</p>
<h4 id="clickhouse高性能存储和查询的实现原理">Clickhouse高性能存储和查询的实现原理</h4>
<p>1、多核CPU并行计算，看到刚才的文件存储方式，文件是按照压缩块的方式存在.bin文件中。ClickHouse可以通过大量CPU，并行读取不同的压缩块并行解压计算。</p>
<p>2、SIMD指令集加速，充分利用了CPU寄存器的并行计算能力。如果是传统的CPU指令集，寄存器就算有128位，如果要进行N次8bit的计算，每次都只能利用寄存器的低8位重复计算N次；而SIMD的话，可以将16个8bit的运算并行放到128位的寄存器中，仅通过1次计算就可以并行计算16个8bit的运算。ClickHouse是通过SSE2指令集实现的，所以最好选用Intel的机器。</p>
<p>3、之前介绍的，ClickHouse分布式水平扩展集群的能力，也可以很好的提升性能。</p>
<p>4、还有稀疏索引，列式存储和极致的数据压缩，也都是大数据场景下高性能查询的关键点。</p>
<p>5、实际上，ClickHouse还有很多优化性能的细节。比如聚合分析，通常的实现是通过HashMap实现，HashMap的key是group by的key，HashMap的value是聚合的值。ClickHouse通过对长字符串的Key，进行Hash实现的HashMapWithSavedHash，以及对Uint8这类范围小的数据字段通过数组实现的FixedHashMap，以及大量新增类别字段和内存限制的场景，也有TwoLevelHashMap和Split to disk的实现方案。</p>
<p>实际上，ClickHouse的内核实现中，并没有使用什么神秘的算法，但是正是这所有的优化组合在一起，才有了ClickHouse彪悍的查询性能。</p>
<h4 id="海量数据直接写入clickhouse会失败">海量数据直接写入ClickHouse会失败</h4>
<p>ClickHouse的MergeTree表引擎，底层原理是类似于LSM-tree。数据通过Append的方式写入，后续再启动merge线程将小的数据文件进行合并。</p>
<p>一次数据写入，会生成一个文件目录。目录结构可以看到，分为四个部分：</p>
<p>第一部分，是分区ID的信息；</p>
<p>第二部分，是这个目录中包含数据的最小BlockNum；</p>
<p>第三部分，是这个目录中包含数据的最大BlockNum；</p>
<p>第四部分，是这个目录进行合并的等级。</p>
<p>举个例子，图中两个黄色的数据目录合并成蓝色的数据目录，数据BlockNum从1_1和2_2合并成了1_2，数据合并level也从0变成了1。</p>
<p>了解了ClickHouse MergeTree家族表的写入过程，这里我们就会发现两个问题。</p>
<p>第一：如果一次写入的数据量太小，比如一条写一次，那么会产生大量的文件目录。当后台合并线程来不及合并的时候，文件目录数量会越来越多，这会导致ClickHouse抛出Too many parts的异常，写入失败。
第二：根据之前的介绍，每一次写入除了写入数据本身，ClickHouse还需要跟Zookeeper进行十来次的数据交互，而我们知道Zookeeper是不能承受高并发的访问。可以看到，写入QPS过高导致进一步Zookeeper的QPS过高，从而导致Zookeeper崩溃。
我们采用的解决方案是，改用Batch的方式写入。即一个Batch的数据，产生一个数据目录，与Zookeeper进行一系列交互。那Batch设置多大呢，Batch太小的话缓解不了ZK的压力，Batch也不能太大，不然上游内存的压力和数据延迟会太大。通过实验，最终我们选用了大小几十万的Batch。</p>
<p>这样避免了QPS太高带来的问题。当前方案其实还有优化空间的，比如Zookeeper是无法线性扩展的。我了解到，业内有些团队就把log和data part相关信息不写入Zookeeper，这样减少了Zookeeper的压力。不过这样涉及到了对源代码的修改，对于一般的业务团队实现的成本太高了。</p>
<p>数据写入，遇到的第二个问题是，如果数据写入通过分布式表写入会遇到单点问题。</p>
<p>先介绍一下分布式表。分布式表实际上是一张逻辑表并不存储真实数据，可以理解为一张代理表。</p>
<p>比如用户查询分布式表，分布式表会将查询请求下发到每一个分片的本地表上进行查询，然后收集每个分片本地表的结果，汇总之后再返回给用户。</p>
<p>用户写入分布式表的场景，是用户将一个Batch的数据写入分布式表，分布式表根据一定的规则，将这个Batch的数据分为若干个Mini Batch的数据，存储到不同的分片上。</p>
<p>这里有一个很容易误解的地方，我们开始以为，分布式表是按照一定规则做一个网络转发，那么我们当时想只要万兆网卡的带宽足够，就不会出现单点的性能瓶颈。</p>
<p>尤其是Clickhouse底层运用的是Mergetree，在合并的过程中，会存在写放大的问题加重磁盘的压力。峰值每分钟几千万条数据写完耗时几十秒，如果正在做Merge就会阻塞写入请求，查询也会非常慢。</p>
<p>我们做的两个优化方案：</p>
<p>第一，对磁盘做Raid提升磁盘的IO。</p>
<p>第二，在写入之前，上游进行数据划分分表操作，直接分开写入到不同的分片上，磁盘压力直接变为了原来的1/N。</p>
<p>这样很好的避免了磁盘的单点瓶颈。</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
    <p style="position: fixed; bottom: 0; width: 100%;text-align: center; color: gray; font-size: 13px;">Copyright &copy; 2020 Jack Wu All Rights Reserved.</p>
  </body>
</html>
